# Advanced Database Systems

> [Course Website](https://15721.courses.cs.cmu.edu/spring2023/)

## Background

> [CMU 15-445/645](https://15445.courses.cs.cmu.edu/fall2022/) or other intro course on databases.

## Course Objectives

Learn about modern practices in database internals and systems programming for analytical workloads.
- Writing correct + performant code
- Proper documentation + testing
- Code reviews
- Working on a large code base

## Course Topics

- Storage Models, Compression
- Indexing
- Vectorized Execution + Compilation
- Parallel Join Algorithms
- Networking Protocols
- Query Optimization
- Modern System Analysis


## Lecture 1: History of Databases

> [Lecture Video](https://www.youtube.com/watch?v=LWS8LEQAUVc&list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&index=1)

> [Lecture Slides](https://15721.courses.cs.cmu.edu/spring2023/slides/01-history.pdf)

> - [What Goes Around Comes Around](https://15721.courses.cs.cmu.edu/spring2023/papers/01-intro/whatgoesaround-stonebraker.pdf)


### 1.1 History Repeats Itself

Old database issues are still relevant today. Many of the ideas in today's database systems are not new.

Somebody invents a "SQL Replacement" every decade. It then fails and/or SQL absorbs the key ideas into the standard.

### 1.2 Integrated Data Store (IDS)

**I**ntegrated **D**ata **S**tore (IDS) was a system developed internally in the 1960s at GE. GE sold their computing division to Honeywell in 1669.

### 1.3 Codasyl

[COBOL](https://en.wikipedia.org/wiki/COBOL) people got together and created **C**onference **O**n **D**ata **S**ystems **L**anguages (CODASYL) in 1969. They created a standard for how programs will access a database.

- Network data model
- Tuple-at-a-time queries

### 1.4 Network Data Model

Suppose you are a buyer and you need to purchase parts from different suppliers.

![image-20230529201314224](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230529201314224.png)

![image-20230529201744678](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230529201744678.png)

The red arrow means pointer which is location of this record on disk or in memory.

### 1.5 Information Management System (IMS)

IBM created **I**nformation **M**anagement **S**ystem (IMS) in 1966. It was built for Apollo moon mission to keep track of all the parts that NASA buying to build rockets. It's still used today in ATM machine or anything on the bank.

### 1.6 Hierarchical Data Model

**Hierarchical Data Model** Provides an interface for programmers to specify which data structure to use for data storage, so when you want to perform a range lookup on the hash table, you must dump all the data and load it back under the new data structure. And programmers need to rewrite application code due to physical data layout changes. **There is a strong coupling between the physical layer and the logical layer**.

![image-20230529203023807](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230529203023807.png)

### 1.7 Relational Model

Ted Codd was a mathematician at IBM. He wrote a paper in 1970 called "A Relational Model of Data for Large Shared Data Banks". He proposed a new way of storing data that is based on set theory and predicate logic.

The relational model has three key part as backbone of all the modern database systems today.
- Store database in simple data structures
- Access data through high-level language (SQL was not invented yet)
- Physical storage left up to implementation

#### 1.7.1 Early implementations of relational DBMS:
- Peterlee Relational Test Vehicle (PRTV) at IBM
- System R at IBM
- Ingres at UC Berkeley
- Oracle
- Mimer

#### 1.7.2 1980s
- IBM first releases SQL/DS in 1981
- IBM then turns out DB2 in 1983
- "SEQUEL" becomes the standard (SQL) after supposedly Stonebraker refused to talk to the ANSI standards committee.

Stonebraker creates `Postgres` as an "object-relational" database system.

> post ingres => Postgres

### 1.8 Object-Relational Model

Avoid "relational-object impedance mismatch" by tightly coupling objects and database.

The reason why object-relational model failed is that `oql` born too late, and there is a tightly coupling between database and programming language.

The problem is that if you want to update the phone field, then you need a big granularity lock to lock the entire phone to prevent concurrency security issues.

![image-20230529215707387](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230529215707387.png)

### 1.9 1990s Boring Days

No major advancements in database systems or application workloads.

- Microsoft forks Sybase and create SQL Server.
- MySQL is written as a replacement for mSQL.
- Postgres gets SQL support.
- SQLite started in early 2000.

Some DBMSs introduced pre-computed data cubes for faster analytics.

### 1.10 2000s Data Warehouses

Rise of the special purpose OLAP DBMSs.
- Distributed / Shared-Nothing
- Relational / SQL
- Usually closed-source

Significant performance benefits from using **columnar data storage** model.

### 1.11 2000s MapReduce Systems

Distributed programming and execution model for analyzing large data sets.
- First proposed by Google in 2004 (**MapReduce**).
- Yahoo! created an open-source implementation called **Hadoop**.
- Data model decided by user-written functions.

### 1.12 2000s NoSQL Systems

Focus on high-availability & high-scalability:
- Schema-less
- Non-relational data models (document, key-value, column-family, etc.)
- No ACID transactions
- Custom API instead of SQL
- Usually open-source

### 1.13 2010s NewSQL Systems

Provide same performance for OLTP workloads as NoSQL DBMSs without giving up ACID:
- Relational / SQL
- Distributed

Almost all the first group of systems failed. Second wave of "distributed SQL" systems are (potentially) doing better.

### 1.14 2010s Cloud Systems

First database-as-a-service (DBaaS) offerings were "containerized" versions of existing DBMSs.

There are new DBMSs that are designed from scratch explicitly for running in a cloud environment.

> Designed for cloud => Cloud-native

### 1.15 Shared-Disk Engine

Instead of writing a custom storage manager, the DBMS leverages distributed storage.
- Scale execution layer independently of storage.
- Favors log-stuctured approaches.

This is what most people think of when they talk about a **data lake**.

### 1.16 Graph Systems

Systems for storing and querying graph-structured data.
- Similar to the nerwork data model (CODASYL).

Their (supposed) advantage over other models is to provide a graph-centric query API. However, [SQL:2023](https://homepages.cwi.nl/~boncz/edbt2022.pdf) is adding graph query syntax (SQL/PCG).

Latest [reasearch](https://www.cidrdb.org/cidr2023/papers/p66-wolde.pdf) shows that a relational DBMS outperforms SOTA graph DBMSs.

### 1.17 Time Series Systems

Specialized systems that are designed to store time series / event data.

This design of these systems make deep assumptions about the distribution of the data and workload query patterns.

> Andy thinks `ClickHouse` will become a big player in future.

### 1.18 Blockchain Databases

Decentralized distributed log with incremental checksums ([Merkle Trees](https://en.wikipedia.org/wiki/Merkle_tree)).
- Uses Byzantine Fault Tolerant (BFT) protocol to determine next entry to append to log.

> Andy is not aware of a blockchain usecase that could not also be solved with a "traditional" OLTP DBMS and/or external policies (e.g., authentication).

### 1.19 Specialized Systems

- Embedded DBMSs
- Multi-Model DBMSs
- Hardware Acceleration 
    > During last 40,50 years, people search for hardware acceleration such as gpus or others, but it never pans out due to never gets the adoption in the market
- Array / Matrix / Vector DBMSs
    > You wouldn't want to use a relational DBMS, you want to use a specialized DBMS for this kind of data. We will see whether the market is big enough to justify needing a specialized DBMS. It could be the next thing.

### 1.20 Summary

SQL is considered the way to go forward right now again. An example would be all the nosql systems from 10 years ago, except for Redis, all are died or support SQL. The relation model has stood the test of time.

## Lecture 2: Modern Analytical Database Systems

> [Lecture Video](https://www.youtube.com/watch?v=7V1oi_8uvuM&list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&index=2)

> [Lecture Slides](https://15721.courses.cs.cmu.edu/spring2023/slides/02-modernolap.pdf)

> - [Building An Elastic Query Engine on Disaggregated Storage](https://15721.courses.cs.cmu.edu/spring2023/papers/02-modern/vuppalapati-nsdi22.pdf)

### 2.1 Distributed Query Execution

Executing an OLAP query in a distributed DBMS is roughly the same as on a single-node DBMS.
- Query plan is a DAG (有向无环图) of physical operators.

For each operator, the DBMS considers where input is coming from and where to send output.
- Table Scans
- Joins
- Aggregations
- Sorting

**Persistent Data**

- The "source of record" for the database (e.g., tables)
- Modern systems assume that these data files are immutable but can support updates by rewriting them. 
    > 就像阿里云OSS上的文件只支持覆盖以实现更新，而不支持修改，顺带一提，在[Building An Elastic Query Engine on Disaggregated Storage](https://15721.courses.cs.cmu.edu/spring2023/papers/02-modern/vuppalapati-nsdi22.pdf)这篇论文中提到[Amazon S3](https://aws.amazon.com/cn/s3/)支持部分读的功能。

**Intermediate Data**

- Short-lived artifacts produced by query operators during execution and then consumed by other operators.
- The amount of intermediate data that a query generates has little to no correlation to amount of persistent data that it reads or the execution time.

### 2.2 Distributed System Architecture

A distributed DBMS's system architecture specifies the location of the database's persistent data files. This affects how nodes coordinate with each other and where they retrieve/store objects in the database.

Two approaches (not mutually exclusive):
- Push Query to Data
- Pull Data to Query

### 2.3 Push vs. Pull

#### 2.3.1 Push Query to Data

- Send the query (or a portion of it, such as some operators) to the node that contains the data.
- Perform as mush filtering and processing as possible where data resides before transmitting over network.

#### 2.3.2 Pull Data to Query

- Bring the data to the node that is executing a query that needs it for processing.
- This is necessary when there is no compute resources available where persistent data files are located. 
    > 你不能把计算都交给阿里云OSS对吧？所以你需要把数据拉到计算节点上进行计算。但实际上你可以[在阿里云OSS上进行简单的SELECT](https://help.aliyun.com/document_detail/106082.html)，[Amazon S3 也提供了这样一个功能](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html)。

### 2.4 Distributed System Architecture

#### 2.4.1 Shared Nothing Architecture

> 一种传统的分布式数据库管理系统架构，MySQL集群就是这种架构（大概？）

Each DBMS instance has its own CPU, memory, locally-attached disk.
- Nodes only communicate with each other via network.

Database is partitioned into disjoint subsets across nodes.
- Adding a new node requires physically moving data between nodes.

Since data is local, the DBMS can access it via POSIX API.

**Advantages**:
- Potentially better performance && efficiency.
- Apply filters where the data resides before transferring over network.

**Disadvantages**:
- Harder to scale capacity (data movement).

#### 2.4.2 Shared Disk Architecture

> snowflake就是这个架构

Each node accesses a single logical disk via an interconnect, but also have their own private memory and ephemeral storage.
- Must send messages between nodes to learn about their current state.

Instead of a POSIX API, the DBMS accesses disk using a userspace API.

Traditionally the storage layer in shared-disk DBMSs were dedicated on-prem NAS.
- Example: Oracle Exadata

Cloud object stores are now the prevailing storage target for modern OLAP DBMSs because they are "infinitely" scalable.
- Examples: Aliyun OSS, Amazon S3, Azure Blob, Google Cloud Storage

**Advantages**:

- Scale compute layer independently from the storage layer.
- Easy to shutdown idle compute layer resources.

**Disadvantages**:
- May need to pull uncached persistent data from storage layer to compute layer before applying filters.

### 2.5 Object Stores

Partition the database's tables (persistent data) into large, immutable files stored in an object store.
- All attributes for a tuple are stored in the same file in a columnar layout (PAX).
- Header (or footer) contains meta-data about columnar offsets, compression schemes, indexes, and zone maps.

The DBMS retrieves a block's header to determine what byte ranges it needs to retrieve (if any).

Each cloud vendor provides their own proprietary API to access data (PUT, GET, DELETE).
- Some vendors support predicate pushdown (Aliyun OSS, S3).

There are some addtional topic:
- File formats (PAX)
- Table partitioning
- Data Ingestion / Updates / Discovery
- Scheduling / Adaptivity

### 2.6 System Catalogs 

A DBMS tracks a database's schema (table, columns) and data files in its catalog.

- If the DBMS is on the data ingestion path, then it can maintain the catalog incrementally.
- If an external process adds data files, then it also needs to update the catalog so that the DBMS is aware of them.

Notable implementations:
- [HCatalog](https://cwiki.apache.org/confluence/display/HCATALOG) (from Hive and open-sourced)
- Google Data Catalog
- Amazon Glue Data Catalog

### 2.7 Query Optimizers

Extendible search engine framework for heuristic and cost-based query optimization.
- DBMS provides transformation rules and cost estimates.
- Framework returns either a logical or physical query plan.

> This is the hardest part to build in any DBMS. 

Notable implementations:
- [Apache Calcite](https://calcite.apache.org/) (Java and Used more widely)
- [Greenplum Qrca](https://github.com/greenplum-db/gporca) (C++)

### 2.8 File Formats

Most DBMSs use a proprietary on-disk binary file format for their databases. The only way to share data between systems is to convert data into a common text-based format, such as **CSV, JSON, XML**.

There are open-source binary file formats that make it easier to access data across systems and libraries for extracting data from files. Libraries provide an iterator interface to retrieve (batched) columns from files.
![image-20230531001916588](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Images/image-20230531001916588.png)

### 2.9 Execution Engines

Standalone libraries for executing vectorized query operators on columnar data.
- Input is a DAG (有向无环图) of physical operators.
- Require external scheduling and orchestration.

> It may not the hardest part in OLAP DBMSs (Query Optimizer is the hardest), but it has a huge ifluence on performance.

Notable implementations:
- Velox
- DataFusion
- Intel OAP

### 2.10 Conclusion

Today was about understanding the high-level context of what modern OLAP DBMSs look like.
- Fundamentally these new DBMSs are not different than previous distributed/parallel DBMSs except for the prevalence of a cloud-based object store for shared disk.

Our focus for the rest of the semester will be about state-of-the-art implementations of these systems' components.

## Lecture 3: Storage Models & Data layout

> [Lecture Video](https://www.youtube.com/watch?v=oWaeWlvPCyk&list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&index=3)

> [Lecture Slides](https://15721.courses.cs.cmu.edu/spring2023/slides/03-storage.pdf)

> - [Column-Stores vs. Row-Stores: How Different Are They Really?](https://15721.courses.cs.cmu.edu/spring2023/papers/03-storage/p967-abadi.pdf)

### 3.1 Storage Models

A DBMS's **storage model** specifies how it physically organizes tuples on disk and in memory.

- N-ary storage model (NSM)
- Decomposition storage model (DSM)
- Hybrid storage model (PAX) (lots of modern DBMSs' choice)

#### 3.1.1 N-ary Storage Model (NSM)

The DBMS stores (almost) all the attributes for a tuple contiguously in a single page. (MySQL will store big size varchar filed and text field and so on in a overflow page.)

Ideal for OLTP workloads where txns tend to access individual entities and insert-heavy workloads.
- Use the tuple-at-a-time **iterator processing model**. (Example: Volcano Iterator Model)

NSM database page sizes are typically some constant multiple of hardware page size (4KB).

**Advantages**:
- Fast inserts, updates, and deletes.
- Good for queries that need the entire tuple (OLTP).
- Can use index-oriented physical storage for clustering.

**Disadvantages**:
- Not good for scanning large portions of the table and/or a subset of the attributes.
- Terrible memory locality in access patterns.
- Not ideal for compression because of multiple value domains within a single page.

#### 3.1.2 Decomposition Storage Model (DSM)

> Actually, DSM is columnar storage model in paper.

The DBMS stores a single attribute for all tuples contiguously in a block of data. Ideal for OLAP workloads where read-only queries perform large scans over a subset of the table’s attributes.
- Use a batched vectorized processing model.

File sizes are larger (100s of MBs), but it may still organize tuples within the file into smaller groups.

Store attributes and meta-data (e.g., nulls) in separate arrays of fixed-length values.
- Most systems identify unique physical tuples using offsets into these arrays.
- Need to handle variable-length values…

Maintain a separate file per attribute with a dedicated header area for metadata about entire column.

There are two choices for DSM:
- Fixed-length Offsets. Each value is the same length for an attribute. (Most choices)
- Embedded Tuple Ids. Each value is stored with its tuple id in a column.

![image-20230531161602503](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230531161602503.png)

Using **dictionary compression** to convert repetitive variable-length data into fixed-length values (like int32).

**Advantages**:
- Reduces the amount wasted I/O per query because the DBMS only reads the data that it needs.
- Faster query processing because of increased locality and cached data reuse.
- Better data compression (more on this later)

**Disadvantages**:
- Slow for point queries, inserts, updates, and deletes because of tuple splitting/stitching/reorganization.

#### 3.1.3 Hybrid Storage Model (PAX)

**Partition Attributes Across** (PAX) is a hybrid storage model that vertically partitions attributes within a database page.

> This is what Paraquet and Orc use.

The goal is to get the benefits of faster processing on columnar storage while retaining the spatial locality benefits of row storage.

Horizontally partition rows into groups. Then vertically partition their attributes into columns. Global header contains directory with the offsets to the file's row groups.
- This is stored in the footer if the file is immutable (Parquet, Orc).

Each row group contains its own meta-data header about its contents.

![image-20230531221515255](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230531221515255.png)

Use separate execution engines that are optimized for either NSM or DSM databases.
- Store new data in NSM for fast OLTP.
- Migrate data to DSM for more efficient OLAP.
- Combine query results from both engines to appear as a single logical database to the application.

There are two choices:
- Fractured Mirror. Store a second copy of the database in a PAX layout that is automatically updated.
    - All updates are first entered in NSM then eventually copied into DSM mirror.
    - If the DBMS supports updates, it must invalidate tuples in the DSM mirror.
- Delta Store. Stage updates to the database in an NSM table. A background thread migrates updates from delta store and applies them to DSM data.
    - Batch large chunks and then write them out as a PAX file.

### 3.2 Type Representation

- Integer / BigInt / SmallInt / TinyInt => C/C++ Representation
- Float / Double => IEEE 754 (not exact value)
- Time / Date / Timestamp => 32/64 bit int of (micro/milli) seconds since Unix epoch (UTC 1970-01-01 00:00:00).
- VARCHAR/VARBINARY/TEXT/BLOB
    - Pointer to other location if type is ≥64-bits.
    - Header with length and address to next location (if segmented), followed by data bytes.
    - Most DBMSs use dictionary compression for these.
- Numeric data types with (potentially) arbitrary precision and scale. Used when rounding errors are unacceptable.

![image-20230531223127632](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230531223127632.png)

![image-20230531223150400](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230531223150400.png)

- Null Data Types
    - **Special Values**. Designate a value to represent NULL for a data type (e.g., INT32_MIN)
    - **Null Column Bitmap Header**. Store a bitmap in a centralized header that specifies what attributes are null.
    - **Per Attribute Null Flag**. Store a flag that marks that a value is null. Must use more space than just a single bit because this messes up with word alignment.
### 3.3 Partitioning

The DBMS executes query fragments on each partition and then combines the results to produce a single answer.

The DBMS can partition a database physically (shared nothing) or logically (shared disk) (most in OLAP).

Split a table's tuples into disjoint subsets based on some partitioning key and scheme.
- Choose column(s) that divides the database equally in terms of size, load, or usage.

Partitioning Schemes:
- Hashing (对某列进行哈希)
- Ranges (对某列的值进行分桶)
- Predicates (对某列进行布尔判断)

## Lecture 4: Analytical Database Indexs

> [Lecture Video](https://www.youtube.com/watch?v=lGRAq98ejWs&list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&index=4)

> [Lecture Slides](https://15721.courses.cs.cmu.edu/spring2023/slides/04-olapindexes.pdf)

> - [Column Sketches: A Scan Accelerator for Rapid and Robust Predicate Evaluation](https://15721.courses.cs.cmu.edu/spring2023/papers/04-olapindexes/hentschel-sigmod18.pdf)

### 4.1 Zone Maps

Pre-computed aggregates for the attribute values in a block of tuples. DBMS checks the zone map first to decide whether it wants to access the block.
- Originally called Small Materialized Aggregates (SMA)
- DBMS automatically creates/maintains this meta-data.

![image-20230602113514823](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602113514823.png)

Trade-off between scope vs. filter efficacy.
- If the scope is too large, then the zone maps will be useless.
- If the scope is too small, then the DBMS will spend too much checking zone maps. 

Zone Maps are only useful when the target attribute's position and values are correlated.

### 4.2 Bitmap Indexes

> Bitmap indexs are more common in row-oriented databases than column-oriented OLAP databases.

Store a separate Bitmap for each unique value for an attribute where an offset in the vector corresponds to a tuple.
- The ith position in the Bitmap corresponds to the ith tuple in the table.

Typically segmented into chunks to avoid allocating large blocks of contiguous memory.
- Example: One per row group in PAX.

There are four choices for Bitmap Indexe encoding:
- Equality Encoding. Basic scheme with one Bitmap per unique value.
- Range Encoding. Using one Bitmap per interval instead of per value.
- Hierarchy Encoding. Use a tree to identify empty key ranges.

    ![image-20230602153914418](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602153914418.png)

- Bit-sliced Encoding. Use a Bitmap per bit location across all values.

    ![image-20230602154049255](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602154049255.png)

    ![image-20230602154106980](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602154106980.png)

- Bitwave Encoding.
    Alternative storage layout for columnar databases that is designed for efficient predicate evaluation on compressed data using SIMD.
    - Order-preserving dictionary encoding.
    - Bit-level parallelization.
    - Only require common instructions (no scatter/gather)
    
    ![image-20230602155753971](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602155753971.png)
    
    ![image-20230602155835573](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602155835573.png)

### 4.3 Column Imprints

Store a bitmap that indicates whether there is a bit set at a bit-slice of cache-line values.

![image-20230602161821144](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602161821144.png)

### 4.4 Column Sketches

A variation of range-encoded Bitmaps that uses a smaller sketch codes to indicate that a tuple's value exists in a range.

DBMS must automatically figure out the best mapping of codes.
- Trade-off between distribution of values and compactness.
- Assign unique codes to frequent values to avoid false positives.

![image-20230602162912678](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230602162912678.png)

## Lecture 5: Database Compression

> [Lecture Video](https://www.youtube.com/watch?v=Bpy5hYyHAuQ&list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&index=5)

> [Lecture Slides](https://15721.courses.cs.cmu.edu/spring2023/slides/05-compression.pdf)

> - [Integrating Compression and Execution in Column-Oriented Database Systems](https://15721.courses.cs.cmu.edu/spring2023/papers/05-compression/abadi-sigmod2006.pdf)

**Goal**

- Must Produce fixed-length values.
- Must be a **lossless** scheme.
    > Any kind of lossy compression must be performed at the application level.
- Ideally postpone decompression for as long as possible during query execution.

### 5.1 Compression Granularity

- **Block-level**. Compress a block (e.g. database page (MySQL), RowGroup) of tuples in a table.
- **Tuple-level**. Compress the contents of the entire tuple individually (NSM-only).
- **Attribute-level**. Compress a single attribute value within one tuple. Can target multiple attributes for the same tuple.
    > Example: TEXT, BLOB, etc. field will be compressed in gzip or other compress algorithm (PostgreSQL).
- **Column-level**. Compress multiple values for one or more attributes stored for multiple tuples (DSM-only).

### 5.2 Naive Page Compression

Compress data using a general-purpose algorithm. Scope of compression is only based on the data provided as input.
- LZO (1996), LZ4 (2011), Snappy (2011), Brotli (2013), Oracle OZIP (2014), Zstd (2015)

Considerations
- Computational overhead
- Compress vs. decompress speed

> Maybe you want to apply a combination of different compression level algotithms to our data. For Example, run a Column-level on column and then run a Block-level on the whole block. However, it's not going to make a big difference but you pay the CPU cost.

[MySQL Innodb Compression](https://dev.mysql.com/doc/refman/8.0/en/innodb-compression.html)

The DBMS must decompress data first before it can be read and (potentially) modified.

- Even if the algo uses dictionary compression, the DBMS cannot access the dictionary's contents.
- This limits the practical scope of the compression scheme.

These schemes also do not consider the high-level meaning or semantics of the data.

### 5.3 Native Columnar Compression

- Run-length Encoding
- Dictionary Encoding
- Bitmap Encoding
- Delta Encoding
- Bit Packing

#### 5.3.1 Run-length Encoding

Compress runs of the same value in a single column into triplets:

- The value of the attribute.
- The start position in the column segment.
- The # of elements in the run.

Requires the columns to be sorted intelligently to maximize compression opportunities. Sometimes also called null suppression if the DBMS only tracks empty space.

![image-20230606201902777](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230606201902777.png)

#### 5.3.2 Dictionary Encoding

Replace frequent values with smaller fixed-length codes and then maintain a mapping (dictionary) from the codes to the original values.

- Typically, one code per attribute value.
- Most widely used native compression scheme in DBMSs.

The ideal dictionary scheme supports fast encoding and decoding for both point and range queries.

- When to construct the dictionary?
    - **All At Once**

        - Compute the dictionary for all the tuples at a given point of time.
        - New tuples must use a separate dictionary, or the all tuples must be recomputed.
        - This is easy to do if the file is immutable.
    - Incrementally
      
        - Merge new tuples in with an existing dictionary.
        - Likely requires re-encoding to existing tuples.

- What is the scope of the dictionary?
    - **Block-level**
      
        - Only include a subset of tuples within a single table.
        - DBMS must decompress data when combining tuples from different blocks (e.g., hash table for joins).
    - Table-level

        - Construct a dictionary for the entire table.
        - Better compression ratio, but expensive to update.
    - Multi-Table 
      
        > For example, we could compress primary key and  other table's foreign key in the same dictionary if they are associated.
        
        - Can be either subset or entire tables.
        - Sometimes helps with joins and set operations.
- What data structure do we use for the dictionary?

    - **Array**
        
        - One array of variable length strings and another array with pointers that maps to string offsets.
        - Expensive to update so only usable in immutable files.
        
        ![image-20230606205346603](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230606205346603.png)
        
    - Hash Table
        - Fast and compact.
        - Unable to support range and prefix queries.
    - **B+Tree**
        
        - Slower than a hash table and takes more memory.
        - Can support range and prefix queries.
        
        ![image-20230606205435902](https://raw.githubusercontent.com/Hxhao2000/ImagesBed/master/Imagesimage-20230606205435902.png)

- What encoding scheme to use for the dictionary?

## Project 1 Foreign Data Wrapper

> [Foreign Data Wrapper](https://15721.courses.cs.cmu.edu/spring2023/project1.html)

You will Build a SQL/MED extension (foreign data wrapper) for PostgreSQL for performing simple scans with predicate pushdown on a custom columnar data format.

The goal is to get you familiar with extending Postgres and writing a simple scan operator.
